{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# import all libraries\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.layers.convolutional import Conv1D\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nimport pandas as pd\nimport numpy as np\nimport spacy\nnlp=spacy.load(\"en\")","execution_count":1,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#load the dataset\ntrain=pd.read_csv(\"../input/emoticons/noemoticon.csv\" , encoding= \"latin-1\")\nY_train = train[train.columns[0]]\nX_train = train[train.columns[5]]","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":6,"outputs":[{"output_type":"stream","text":"/kaggle/input/glovetwitter27b100dtxt/glove.twitter.27B.200d.txt\n/kaggle/input/emoticons/noemoticon.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data into test and train\nfrom sklearn.model_selection import train_test_split\ntrainset1x, trainset2x, trainset1y, trainset2y = train_test_split(X_train.values, Y_train.values, test_size=0.02,random_state=42 )\ntrainset2y=pd.get_dummies(trainset2y)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# function to remove stopwords\ndef stopwords(sentence):\n    new=[]\n    sentence=nlp(sentence)\n    for w in sentence:\n        if (w.is_stop == False) & (w.pos_ !=\"PUNCT\"):\n            new.append(w.string.strip())\n        c=\" \".join(str(x) for x in new)\n    return c\n\n# function to lemmatize the tweets\ndef lemmatize(sentence):\n    sentence=nlp(sentence)\n    str=\"\"\n    \n    for w in sentence:\n        str+=\" \"+w.lemma_\n        \n    return nlp(str)\n\n#loading the glove model\ndef loadGloveModel(gloveFile):\n    print(\"Loading Glove Model\")\n    f = open(gloveFile,'r')\n    model = {}\n    for line in f:\n        splitLine = line.split()\n        word = splitLine[0]\n        embedding = [float(val) for val in splitLine[1:]]\n        model[word] = embedding\n    print (\"Done.\"),len(model),(\" words loaded!\")\n    return model\n\n#vectorising the sentences\ndef sent_vectorizer(sent, model):\n    sent_vec = np.zeros(200)\n    numw = 0\n    for w in sent.split():\n        try:\n            sent_vec = np.add(sent_vec, model[str(w)])\n            numw+=1\n        except:\n            pass\n    return sent_vec","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save the glove model\nmodel=loadGloveModel(\"../input/glovetwitter27b100dtxt/glove.twitter.27B.200d.txt\")\n","execution_count":11,"outputs":[{"output_type":"stream","text":"Loading Glove Model\nDone.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#obtain a clean vector\ncleanvector=[]\nfor i in range(trainset2x.shape[0]):\n    document=trainset2x[i]\n    document=document.lower()\n    document=lemmatize(document)\n    document=str(document)\n    cleanvector.append(sent_vectorizer(document,model))","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Getting the input and output in proper shape\ncleanvector=np.array(cleanvector)\ncleanvector =cleanvector.reshape(len(cleanvector),200,1)\n","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tokenizing the sequences\ntokenizer = Tokenizer(num_words=16000)\ntokenizer.fit_on_texts(trainset2x)\nsequences = tokenizer.texts_to_sequences(trainset2x)\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\ndata = pad_sequences(sequences, maxlen=15, padding=\"post\")\nprint(data.shape)","execution_count":14,"outputs":[{"output_type":"stream","text":"Found 1873 unique tokens.\n(400, 15)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reshape the data and preparing to train\ndata=data.reshape(len(cleanvector),15,1)\nfrom sklearn.model_selection import train_test_split\ntrainx, validx, trainy, validy = train_test_split(data, trainset2y, test_size=0.3,random_state=42 )","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#calculate the number of words\nnb_words=len(tokenizer.word_index)+1\n\n#obtain theembedding matrix\nembedding_matrix = np.zeros((nb_words, 200))\nfor word, i in word_index.items():\n    embedding_vector = model.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\nprint('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n\ntrainy=np.array(trainy)\nvalidy=np.array(validy)\n","execution_count":16,"outputs":[{"output_type":"stream","text":"Null word embeddings: 346\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#building a simple RNN model\ndef modelbuild():\n    model = Sequential()\n    model.add(keras.layers.InputLayer(input_shape=(15,1)))\n    keras.layers.embeddings.Embedding(nb_words, 15, weights=[embedding_matrix], input_length=15,\n    trainable=False)\n \n    model.add(keras.layers.recurrent.SimpleRNN(units = 100, activation='relu',\n    use_bias=True))\n    model.add(keras.layers.Dense(units=1000, input_dim = 2000, activation='sigmoid'))\n    model.add(keras.layers.Dense(units=500, input_dim=1000, activation='relu'))\n    model.add(keras.layers.Dense(units=2, input_dim=500,activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    return model","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#compiling the model\nfinalmodel = modelbuild()\nfinalmodel.fit(trainx, trainy, epochs=10, batch_size=120,validation_data=(validx,validy))","execution_count":18,"outputs":[{"output_type":"stream","text":"Train on 280 samples, validate on 120 samples\nEpoch 1/10\n280/280 [==============================] - 2s 6ms/step - loss: 2.0574 - acc: 0.4571 - val_loss: 2.1487 - val_acc: 0.5083\nEpoch 2/10\n280/280 [==============================] - 0s 192us/step - loss: 1.9140 - acc: 0.5321 - val_loss: 0.8098 - val_acc: 0.5333\nEpoch 3/10\n280/280 [==============================] - 0s 192us/step - loss: 0.8492 - acc: 0.4821 - val_loss: 1.0728 - val_acc: 0.4917\nEpoch 4/10\n280/280 [==============================] - 0s 175us/step - loss: 0.8867 - acc: 0.4929 - val_loss: 0.7546 - val_acc: 0.5000\nEpoch 5/10\n280/280 [==============================] - 0s 176us/step - loss: 0.7261 - acc: 0.5643 - val_loss: 0.8541 - val_acc: 0.5167\nEpoch 6/10\n280/280 [==============================] - 0s 182us/step - loss: 0.7393 - acc: 0.5607 - val_loss: 0.7077 - val_acc: 0.5250\nEpoch 7/10\n280/280 [==============================] - 0s 173us/step - loss: 0.6570 - acc: 0.5750 - val_loss: 0.7377 - val_acc: 0.4750\nEpoch 8/10\n280/280 [==============================] - 0s 164us/step - loss: 0.6471 - acc: 0.6107 - val_loss: 0.7269 - val_acc: 0.5333\nEpoch 9/10\n280/280 [==============================] - 0s 172us/step - loss: 0.6208 - acc: 0.6464 - val_loss: 0.7517 - val_acc: 0.5250\nEpoch 10/10\n280/280 [==============================] - 0s 167us/step - loss: 0.6165 - acc: 0.6679 - val_loss: 0.7317 - val_acc: 0.5333\n","name":"stdout"},{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"<keras.callbacks.History at 0x7f7880476748>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"}},"nbformat":4,"nbformat_minor":1}