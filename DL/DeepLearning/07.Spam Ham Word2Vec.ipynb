{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/sms-spam-collection-dataset/spam.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport gensim \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split","execution_count":51,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.DataFrame()\ndf = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\",encoding='latin1')\ndf.head(3)","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"     v1                                                 v2 Unnamed: 2  \\\n0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n1   ham                      Ok lar... Joking wif u oni...        NaN   \n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n\n  Unnamed: 3 Unnamed: 4  \n0        NaN        NaN  \n1        NaN        NaN  \n2        NaN        NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Unnamed: 2</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#df = df.head(3)\ndf.shape","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"(5572, 5)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.v1 = LabelEncoder().fit_transform(df.v1)\n","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"review_lines = list()\nlines = df['v2'].values.tolist()\n\nfor line in lines:   \n    tokens = word_tokenize(line)\n    \n    # convert to lower case\n    tokens = [w.lower() for w in tokens]\n    \n    # remove punctuation from each word    \n    table = str.maketrans('', '', string.punctuation)\n    stripped = [w.translate(table) for w in tokens]\n    \n    # remove remaining tokens that are not alphabetic\n    words = [word for word in stripped if word.isalpha()]\n    \n    # filter out stop words    \n    stop_words = set(stopwords.words('english'))\n    words = [w for w in words if not w in stop_words]\n    review_lines.append(words)","execution_count":36,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(review_lines)\n","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"5572"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM = 100\n# train word2vec model\nmodel = gensim.models.Word2Vec(sentences=review_lines, size=EMBEDDING_DIM, window=5, workers=4, min_count=1)\n#model = gensim.models.Word2Vec(sentences=review_lines)\n# vocab size\nwords = list(model.wv.vocab)\nprint('Vocabulary size: %d' % len(words))","execution_count":39,"outputs":[{"output_type":"stream","text":"Vocabulary size: 7782\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#words","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# save model in ASCII (word2vec) format\nfilename = 'spam_ham_embedding_word2vec.txt'\nmodel.wv.save_word2vec_format(filename, binary=False)\n#/kaggle/working/spam_ham_embedding_word2vec.txt\n","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"<gensim.models.keyedvectors.Word2VecKeyedVectors at 0x7fb48c34b588>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.most_similar('hard')#, topn =1)","execution_count":43,"outputs":[{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"[('life', 0.9970679879188538),\n ('much', 0.9970424175262451),\n ('wat', 0.9970350861549377),\n ('went', 0.9970228672027588),\n ('amp', 0.9969656467437744),\n ('sure', 0.996953010559082),\n ('job', 0.9969454407691956),\n ('getting', 0.996932864189148),\n ('today', 0.9969325065612793),\n ('n', 0.9969313740730286)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Letâ€™s see the result of semantically reasonable word vectors (king - man + woman)\nmodel.wv.most_similar_cosmul(positive=['woman', 'king'], negative=['man'])\n\n#odd word out\nprint(model.wv.doesnt_match(\"woman king queen movie\".split()))","execution_count":47,"outputs":[{"output_type":"stream","text":"queen\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nimport os\n\nembeddings_index = {}\nf = open(os.path.join('', 'spam_ham_embedding_word2vec.txt'),  encoding = \"utf-8\")\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:])\n    embeddings_index[word] = coefs\nf.close()","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape","execution_count":49,"outputs":[{"output_type":"execute_result","execution_count":49,"data":{"text/plain":"(5572, 5)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train = df.loc[:2, 'v2'].values\n# y_train = df.loc[:2, 'v1'].values\n# X_test = df.loc[2:, 'v2'].values\n# y_test = df.loc[2:, 'v1'].values","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(df['v2'], df['v1'], test_size=0.2, random_state = 7)","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape,X_test.shape","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"((4457,), (1115,))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"total_reviews = X_train + X_test\nmax_length = 100 # try other options like mean of sentence lengths","execution_count":59,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\nVALIDATION_SPLIT = 0.2\n\n# vectorize the text samples into a 2D integer tensor\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(review_lines)\nsequences = tokenizer_obj.texts_to_sequences(review_lines)\n\n# pad sequences\nword_index = tokenizer_obj.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\nreview_pad = pad_sequences(sequences, maxlen=100)\nsentiment =  df['v1'].values\nprint('Shape of review tensor:', review_pad.shape)\nprint('Shape of sentiment tensor:', sentiment.shape)\n\n# split the data into a training set and a validation set\nindices = np.arange(review_pad.shape[0])\nnp.random.shuffle(indices)\nreview_pad = review_pad[indices]\nsentiment = sentiment[indices]\nnum_validation_samples = int(VALIDATION_SPLIT * review_pad.shape[0])\n\nX_train_pad = review_pad[:-num_validation_samples]\ny_train = sentiment[:-num_validation_samples]\nX_test_pad = review_pad[-num_validation_samples:]\ny_test = sentiment[-num_validation_samples:]","execution_count":60,"outputs":[{"output_type":"stream","text":"Found 7782 unique tokens.\nShape of review tensor: (5572, 100)\nShape of sentiment tensor: (5572,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Shape of X_train_pad tensor:', X_train_pad.shape)\nprint('Shape of y_train tensor:', y_train.shape)\n\nprint('Shape of X_test_pad tensor:', X_test_pad.shape)\nprint('Shape of y_test tensor:', y_test.shape)","execution_count":61,"outputs":[{"output_type":"stream","text":"Shape of X_train_pad tensor: (4458, 100)\nShape of y_train tensor: (4458,)\nShape of X_test_pad tensor: (1114, 100)\nShape of y_test tensor: (1114,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"EMBEDDING_DIM =100\nnum_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n\nfor word, i in word_index.items():\n    if i > num_words:\n        continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(num_words)","execution_count":63,"outputs":[{"output_type":"stream","text":"7783\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Flatten\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.initializers import Constant\n\n# define model\nmodel = Sequential()\n# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=100,\n                            trainable=False)\n\nmodel.add(embedding_layer)\nmodel.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nprint(model.summary())\n\n# compile network\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n# fit the model\nmodel.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)","execution_count":64,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 100, 100)          778300    \n_________________________________________________________________\nconv1d_2 (Conv1D)            (None, 96, 128)           64128     \n_________________________________________________________________\nmax_pooling1d_2 (MaxPooling1 (None, 48, 128)           0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 6144)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 6145      \n=================================================================\nTotal params: 848,573\nTrainable params: 70,273\nNon-trainable params: 778,300\n_________________________________________________________________\nNone\nTrain on 4458 samples, validate on 1114 samples\nEpoch 1/25\n - 2s - loss: 0.4547 - acc: 0.8437 - val_loss: 0.3330 - val_acc: 0.8528\nEpoch 2/25\n - 2s - loss: 0.3345 - acc: 0.8636 - val_loss: 0.3096 - val_acc: 0.8609\nEpoch 3/25\n - 2s - loss: 0.3219 - acc: 0.8627 - val_loss: 0.3028 - val_acc: 0.8636\nEpoch 4/25\n - 2s - loss: 0.3160 - acc: 0.8632 - val_loss: 0.3075 - val_acc: 0.8618\nEpoch 5/25\n - 2s - loss: 0.3119 - acc: 0.8661 - val_loss: 0.2949 - val_acc: 0.8698\nEpoch 6/25\n - 2s - loss: 0.3093 - acc: 0.8697 - val_loss: 0.3136 - val_acc: 0.8636\nEpoch 7/25\n - 2s - loss: 0.3079 - acc: 0.8659 - val_loss: 0.2919 - val_acc: 0.8707\nEpoch 8/25\n - 2s - loss: 0.3014 - acc: 0.8737 - val_loss: 0.2897 - val_acc: 0.8689\nEpoch 9/25\n - 2s - loss: 0.2999 - acc: 0.8721 - val_loss: 0.2896 - val_acc: 0.8725\nEpoch 10/25\n - 2s - loss: 0.2962 - acc: 0.8739 - val_loss: 0.2839 - val_acc: 0.8752\nEpoch 11/25\n - 2s - loss: 0.2916 - acc: 0.8766 - val_loss: 0.2837 - val_acc: 0.8752\nEpoch 12/25\n - 2s - loss: 0.2921 - acc: 0.8746 - val_loss: 0.2806 - val_acc: 0.8770\nEpoch 13/25\n - 2s - loss: 0.2896 - acc: 0.8766 - val_loss: 0.2782 - val_acc: 0.8779\nEpoch 14/25\n - 2s - loss: 0.2901 - acc: 0.8760 - val_loss: 0.2772 - val_acc: 0.8824\nEpoch 15/25\n - 2s - loss: 0.2849 - acc: 0.8802 - val_loss: 0.2773 - val_acc: 0.8806\nEpoch 16/25\n - 2s - loss: 0.2819 - acc: 0.8784 - val_loss: 0.2837 - val_acc: 0.8743\nEpoch 17/25\n - 2s - loss: 0.2792 - acc: 0.8809 - val_loss: 0.2782 - val_acc: 0.8788\nEpoch 18/25\n - 2s - loss: 0.2724 - acc: 0.8860 - val_loss: 0.2736 - val_acc: 0.8869\nEpoch 19/25\n - 2s - loss: 0.2691 - acc: 0.8883 - val_loss: 0.2768 - val_acc: 0.8824\nEpoch 20/25\n - 2s - loss: 0.2686 - acc: 0.8887 - val_loss: 0.2673 - val_acc: 0.8869\nEpoch 21/25\n - 2s - loss: 0.2667 - acc: 0.8883 - val_loss: 0.2661 - val_acc: 0.8887\nEpoch 22/25\n - 2s - loss: 0.2608 - acc: 0.8937 - val_loss: 0.2681 - val_acc: 0.8923\nEpoch 23/25\n - 2s - loss: 0.2587 - acc: 0.8930 - val_loss: 0.2761 - val_acc: 0.8815\nEpoch 24/25\n - 2s - loss: 0.2539 - acc: 0.8959 - val_loss: 0.2606 - val_acc: 0.8959\nEpoch 25/25\n - 2s - loss: 0.2570 - acc: 0.8903 - val_loss: 0.2618 - val_acc: 0.8905\n","name":"stdout"},{"output_type":"execute_result","execution_count":64,"data":{"text/plain":"<keras.callbacks.History at 0x7fb47c0a4710>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# evaluate the model\nloss, accuracy = model.evaluate(X_test_pad, y_test, batch_size=128)\nprint('Accuracy: %f' % (accuracy*100))","execution_count":65,"outputs":[{"output_type":"stream","text":"1114/1114 [==============================] - 0s 131us/step\nAccuracy: 89.048474\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Let us test some  samples\n# load the dataset but only keep the top n words, zero the rest\n\ntest_sample_1 = \"This movie is fantastic! I really like it because it is so good!\"\ntest_sample_2 = \"Good movie!\"\ntest_sample_3 = \"Maybe I like this movie.\"\ntest_sample_4 = \"Not to my taste, will skip and watch another movie\"\ntest_sample_5 = \"if you like action, then this movie might be good for you.\"\ntest_sample_6 = \"Bad movie!\"\ntest_sample_7 = \"Not a good movie!\"\ntest_sample_8 = \"This movie really sucks! Can I get my money back please?\"\ntest_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8]\n\ntest_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\ntest_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=3\n                                       )\n\n#predict\nmodel.predict(x=test_samples_tokens_pad)","execution_count":66,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"Error when checking input: expected embedding_2_input to have shape (100,) but got array with shape (3,)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-66-211e04a9455c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_samples_tokens_pad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    136\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Error when checking input: expected embedding_2_input to have shape (100,) but got array with shape (3,)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, GRU\nfrom keras.layers.embeddings import Embedding\nfrom keras.initializers import Constant\n\n# define model\nmodel = Sequential()\nembedding_layer = Embedding(num_words,\n                            EMBEDDING_DIM,\n                            embeddings_initializer=Constant(embedding_matrix),\n                            input_length=100,\n                            trainable=False)\nmodel.add(embedding_layer)\nmodel.add(GRU(units=32,  dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n# try using different optimizers and different optimizer configs\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint('Summary of the built model...')\nprint(model.summary())","execution_count":67,"outputs":[{"output_type":"stream","text":"Summary of the built model...\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (None, 100, 100)          778300    \n_________________________________________________________________\ngru_1 (GRU)                  (None, 32)                12768     \n_________________________________________________________________\ndense_3 (Dense)              (None, 1)                 33        \n=================================================================\nTotal params: 791,101\nTrainable params: 12,801\nNon-trainable params: 778,300\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train...')\n\nmodel.fit(X_train_pad, y_train, batch_size=128, epochs=25, validation_data=(X_test_pad, y_test), verbose=2)","execution_count":68,"outputs":[{"output_type":"stream","text":"Train...\nTrain on 4458 samples, validate on 1114 samples\nEpoch 1/25\n - 4s - loss: 0.4999 - acc: 0.8587 - val_loss: 0.4605 - val_acc: 0.8627\nEpoch 2/25\n - 3s - loss: 0.4454 - acc: 0.8668 - val_loss: 0.4427 - val_acc: 0.8627\nEpoch 3/25\n - 3s - loss: 0.4288 - acc: 0.8668 - val_loss: 0.4196 - val_acc: 0.8627\nEpoch 4/25\n - 3s - loss: 0.3939 - acc: 0.8668 - val_loss: 0.3551 - val_acc: 0.8627\nEpoch 5/25\n - 3s - loss: 0.3500 - acc: 0.8652 - val_loss: 0.3258 - val_acc: 0.8627\nEpoch 6/25\n - 3s - loss: 0.3419 - acc: 0.8620 - val_loss: 0.3252 - val_acc: 0.8627\nEpoch 7/25\n - 3s - loss: 0.3330 - acc: 0.8652 - val_loss: 0.3169 - val_acc: 0.8627\nEpoch 8/25\n - 3s - loss: 0.3315 - acc: 0.8661 - val_loss: 0.3139 - val_acc: 0.8627\nEpoch 9/25\n - 3s - loss: 0.3272 - acc: 0.8665 - val_loss: 0.3087 - val_acc: 0.8627\nEpoch 10/25\n - 3s - loss: 0.3233 - acc: 0.8659 - val_loss: 0.3051 - val_acc: 0.8627\nEpoch 11/25\n - 3s - loss: 0.3194 - acc: 0.8641 - val_loss: 0.3054 - val_acc: 0.8627\nEpoch 12/25\n - 3s - loss: 0.3205 - acc: 0.8654 - val_loss: 0.3012 - val_acc: 0.8627\nEpoch 13/25\n - 3s - loss: 0.3187 - acc: 0.8668 - val_loss: 0.3047 - val_acc: 0.8627\nEpoch 14/25\n - 3s - loss: 0.3159 - acc: 0.8636 - val_loss: 0.3021 - val_acc: 0.8627\nEpoch 15/25\n - 3s - loss: 0.3183 - acc: 0.8643 - val_loss: 0.3049 - val_acc: 0.8627\nEpoch 16/25\n - 3s - loss: 0.3124 - acc: 0.8668 - val_loss: 0.2978 - val_acc: 0.8627\nEpoch 17/25\n - 3s - loss: 0.3127 - acc: 0.8654 - val_loss: 0.2968 - val_acc: 0.8627\nEpoch 18/25\n - 3s - loss: 0.3136 - acc: 0.8638 - val_loss: 0.2977 - val_acc: 0.8627\nEpoch 19/25\n - 3s - loss: 0.3095 - acc: 0.8656 - val_loss: 0.2938 - val_acc: 0.8645\nEpoch 20/25\n - 3s - loss: 0.3093 - acc: 0.8683 - val_loss: 0.2947 - val_acc: 0.8627\nEpoch 21/25\n - 3s - loss: 0.3052 - acc: 0.8647 - val_loss: 0.2964 - val_acc: 0.8645\nEpoch 22/25\n - 3s - loss: 0.3076 - acc: 0.8659 - val_loss: 0.3007 - val_acc: 0.8627\nEpoch 23/25\n - 3s - loss: 0.3115 - acc: 0.8659 - val_loss: 0.2931 - val_acc: 0.8654\nEpoch 24/25\n - 3s - loss: 0.3076 - acc: 0.8670 - val_loss: 0.2914 - val_acc: 0.8654\nEpoch 25/25\n - 3s - loss: 0.3052 - acc: 0.8632 - val_loss: 0.2910 - val_acc: 0.8654\n","name":"stdout"},{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"<keras.callbacks.History at 0x7fb4587c5a90>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Testing...')\nscore, acc = model.evaluate(X_test_pad, y_test, batch_size=128)\n\nprint('Test score:', score)\nprint('Test accuracy:', acc)\n\nprint(\"Accuracy: {0:.2%}\".format(acc))","execution_count":69,"outputs":[{"output_type":"stream","text":"Testing...\n1114/1114 [==============================] - 0s 173us/step\nTest score: 0.2910021193579669\nTest accuracy: 0.8653500905156778\nAccuracy: 86.54%\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}