{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n#https://nlpforhackers.io/keras-intro/","execution_count":75,"outputs":[{"output_type":"stream","text":"/kaggle/input/sms-spam-collection-dataset/spam.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.utils.vis_utils import plot_model","execution_count":76,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM, Embedding","execution_count":77,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_review(text):\n    # Strip HTML tags\n    text = re.sub('<[^<]+?>', ' ', text)\n \n    # Strip escaped quotes\n    text = text.replace('\\\\\"', '')\n \n    # Strip quotes\n    text = text.replace('\"', '')\n \n    return text","execution_count":78,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = pd.read_csv(\"../input/sms-spam-collection-dataset/spam.csv\",encoding='latin1')","execution_count":79,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(data.shape)\ndata.head()","execution_count":80,"outputs":[{"output_type":"stream","text":"(5572, 5)\n","name":"stdout"},{"output_type":"execute_result","execution_count":80,"data":{"text/plain":"     v1                                                 v2 Unnamed: 2  \\\n0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n1   ham                      Ok lar... Joking wif u oni...        NaN   \n2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n3   ham  U dun say so early hor... U c already then say...        NaN   \n4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n\n  Unnamed: 3 Unnamed: 4  \n0        NaN        NaN  \n1        NaN        NaN  \n2        NaN        NaN  \n3        NaN        NaN  \n4        NaN        NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Unnamed: 2</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.drop([\"Unnamed: 2\",\"Unnamed: 3\",\"Unnamed: 4\"], axis = 1)","execution_count":81,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.v1.value_counts()","execution_count":82,"outputs":[{"output_type":"execute_result","execution_count":82,"data":{"text/plain":"ham     4825\nspam     747\nName: v1, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.v1 = LabelEncoder().fit_transform(data.v1)\ndata.head()\ndata.v1.value_counts()\n","execution_count":83,"outputs":[{"output_type":"execute_result","execution_count":83,"data":{"text/plain":"0    4825\n1     747\nName: v1, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['v2'] = data['v2'].apply(clean_review)\nx_train, x_text, y_train, y_test = train_test_split(data['v2'], data['v1'], test_size=0.2)","execution_count":84,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Count vectorizer"},{"metadata":{"trusted":true},"cell_type":"code","source":"cntvec = CountVectorizer(binary=True, stop_words=stopwords.words('english'), \n                             lowercase=True, min_df=3, max_df=0.9, max_features=5000)\nx_train_onehot = cntvec.fit_transform(x_train)","execution_count":85,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(cntvec.get_feature_names())","execution_count":86,"outputs":[{"output_type":"execute_result","execution_count":86,"data":{"text/plain":"2291"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Keras Simple\nHereâ€™s how to create a simple, 2 layer network. The first layer (which actually comes after an input layer) is called the hidden layer, and the second one is called the output layer"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Dense(units = 500, \n               activation = 'relu',\n               input_dim = len(cntvec.get_feature_names()) ))\n\nmodel.add(Dense(units = 1, activation= 'sigmoid'))\nmodel.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\nmodel.summary()\n#plot_model(model, show_shapes=True, show_layer_names=True)","execution_count":87,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_12 (Dense)             (None, 500)               1146000   \n_________________________________________________________________\ndense_13 (Dense)             (None, 1)                 501       \n=================================================================\nTotal params: 1,146,501\nTrainable params: 1,146,501\nNon-trainable params: 0\n_________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x_train_onehot[:-100], \n          y_train[:-100], \n          epochs=5, \n          batch_size=128, \n          verbose=1, \n          validation_data=(x_train_onehot[-100:], y_train[-100:]))","execution_count":88,"outputs":[{"output_type":"stream","text":"Train on 4357 samples, validate on 100 samples\nEpoch 1/5\n4357/4357 [==============================] - 1s 267us/step - loss: 0.4007 - acc: 0.9243 - val_loss: 0.1341 - val_acc: 0.9800\nEpoch 2/5\n4357/4357 [==============================] - 1s 146us/step - loss: 0.1050 - acc: 0.9814 - val_loss: 0.0555 - val_acc: 0.9900\nEpoch 3/5\n4357/4357 [==============================] - 1s 145us/step - loss: 0.0468 - acc: 0.9906 - val_loss: 0.0438 - val_acc: 0.9900\nEpoch 4/5\n4357/4357 [==============================] - 1s 148us/step - loss: 0.0271 - acc: 0.9940 - val_loss: 0.0388 - val_acc: 0.9900\nEpoch 5/5\n4357/4357 [==============================] - 1s 146us/step - loss: 0.0175 - acc: 0.9968 - val_loss: 0.0378 - val_acc: 0.9900\n","name":"stdout"},{"output_type":"execute_result","execution_count":88,"data":{"text/plain":"<keras.callbacks.History at 0x7f717b191160>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model.evaluate(cntvec.transform(x_text), y_test, verbose=1)\nprint(\"Accuracy:\", scores[1])  # Accuracy: 0.875","execution_count":89,"outputs":[{"output_type":"stream","text":"1115/1115 [==============================] - 0s 80us/step\nAccuracy: 0.9901345291479821\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"> ### LSTM"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenize","execution_count":90,"outputs":[{"output_type":"execute_result","execution_count":90,"data":{"text/plain":"<function sklearn.feature_extraction.text.VectorizerMixin.build_tokenizer.<locals>.<lambda>(doc)>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"word2idx = {word: idx for idx, word in enumerate(cntvec.get_feature_names())}\ntokenize = cntvec.build_tokenizer()\npreprocess = cntvec.build_preprocessor()\n \ndef to_sequence(tokenizer, preprocessor, index, text):\n    words = tokenizer(preprocessor(text))\n    indexes = [index[word] for word in words if word in index]\n    return indexes\n \nprint(to_sequence(tokenize, preprocess, word2idx, \"This is an important test!\"))  # [2269, 4453]\nX_train_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in x_train]\nprint(X_train_sequences[0])","execution_count":91,"outputs":[{"output_type":"stream","text":"[1014, 1970]\n[1417, 924, 305, 1156]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compute the max lenght of a text\nMAX_SEQ_LENGHT = len(max(X_train_sequences, key=len))\nprint(\"MAX_SEQ_LENGHT=\", MAX_SEQ_LENGHT)\n \nfrom keras.preprocessing.sequence import pad_sequences\nN_FEATURES = len(cntvec.get_feature_names())\nX_train_sequences = pad_sequences(X_train_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\nprint(X_train_sequences[0])\nprint(X_train_sequences[1000])\n ","execution_count":92,"outputs":[{"output_type":"stream","text":"MAX_SEQ_LENGHT= 59\n[2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291\n 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291\n 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291\n 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 1417\n  924  305 1156]\n[2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291\n 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291\n 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291 2291\n 2291 2291  814 2032 1530 2028  394 1358 1600  860 1743  711 2183 1971\n 1475 2096   25]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(len(cntvec.get_feature_names()) + 1,\n                    64,  # Embedding size\n                    input_length=MAX_SEQ_LENGHT))\nmodel.add(LSTM(64))\nmodel.add(Dense(units=1, activation='sigmoid'))\n \nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":93,"outputs":[{"output_type":"stream","text":"_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_2 (Embedding)      (None, 59, 64)            146688    \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 64)                33024     \n_________________________________________________________________\ndense_14 (Dense)             (None, 1)                 65        \n=================================================================\nTotal params: 179,777\nTrainable params: 179,777\nNon-trainable params: 0\n_________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(X_train_sequences[:-100], y_train[:-100], \n          epochs=5, batch_size=128, verbose=1, \n          validation_data=(X_train_sequences[-100:], y_train[-100:]))","execution_count":97,"outputs":[{"output_type":"stream","text":"Train on 4357 samples, validate on 100 samples\nEpoch 1/5\n4357/4357 [==============================] - 3s 574us/step - loss: 0.0589 - acc: 0.9846 - val_loss: 0.0784 - val_acc: 0.9800\nEpoch 2/5\n4357/4357 [==============================] - 2s 570us/step - loss: 0.0327 - acc: 0.9908 - val_loss: 0.0624 - val_acc: 0.9900\nEpoch 3/5\n4357/4357 [==============================] - 2s 572us/step - loss: 0.0192 - acc: 0.9961 - val_loss: 0.0623 - val_acc: 0.9900\nEpoch 4/5\n4357/4357 [==============================] - 3s 577us/step - loss: 0.0124 - acc: 0.9975 - val_loss: 0.0612 - val_acc: 0.9900\nEpoch 5/5\n4357/4357 [==============================] - 3s 620us/step - loss: 0.0091 - acc: 0.9982 - val_loss: 0.0674 - val_acc: 0.9900\n","name":"stdout"},{"output_type":"execute_result","execution_count":97,"data":{"text/plain":"<keras.callbacks.History at 0x7f717ae182b0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_sequences = [to_sequence(tokenize, preprocess, word2idx, x) for x in x_text]\nX_test_sequences = pad_sequences(X_test_sequences, maxlen=MAX_SEQ_LENGHT, value=N_FEATURES)\n ","execution_count":98,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scores = model.evaluate(X_test_sequences, y_test, verbose=1)\nprint(\"Accuracy:\", scores[1]) # 0.989","execution_count":99,"outputs":[{"output_type":"stream","text":"1115/1115 [==============================] - 0s 373us/step\nAccuracy: 0.989237668161435\n","name":"stdout"}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}